{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Available modes Pose, Depth, HED, Canny, Seg\n",
    "import cv2\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n",
    "\n",
    "controlnet_libs = {\n",
    "    'pose': \"lllyasviel/sd-controlnet-openpose\",\n",
    "    'scribble': \"lllyasviel/sd-controlnet-scribble\",\n",
    "    'canny': \"lllyasviel/sd-controlnet-canny\",\n",
    "    'hed': \"lllyasviel/sd-controlnet-hed\",\n",
    "    'depth': \"lllyasviel/sd-controlnet-depth\"\n",
    "}\n",
    "\n",
    "def load_pipeline(mode_1, mode_2,device='cuda'):\n",
    "    controlnet_1 = ControlNetModel.from_pretrained(controlnet_libs[mode_1])\n",
    "    controlnet_2 = ControlNetModel.from_pretrained(controlnet_libs[mode_2])\n",
    "\n",
    "    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet_2\n",
    "    )\n",
    "\n",
    "    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "\n",
    "    pipe.controlnet1=controlnet_1.to(device)\n",
    "    pipe.controlnet2= controlnet_2.to(device)\n",
    "\n",
    "    pipe=pipe.to(device)\n",
    "    pipe.enable_xformers_memory_efficient_attention()\n",
    "\n",
    "    return pipe\n",
    "\n",
    "\n",
    "mode_1 = 'pose'\n",
    "mode_2 = 'depth'\n",
    "\n",
    "\n",
    "pipe= load_pipeline(mode_1, mode_2)\n",
    "\n",
    "prompt = \"Spiderman waving from times square\"\n",
    "prompts=[prompt]\n",
    "negative_prompts=[\" monochrome, bad anatomy, lowres,  worst quality, low quality\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from img_process_utils import find_mode\n",
    "\n",
    "input_type = \"image\" # or \"mode\"\n",
    "\n",
    "if input_type==\"image\": \n",
    "    image1=load_image(\"./test_images/luffy.jpg\")\n",
    "    image2=load_image(\"./test_images/times.jpeg\")\n",
    "    control1 = find_mode(mode_1,image1)\n",
    "    control2 = find_mode(mode_2,image2)\n",
    "else:\n",
    "    control1=load_image(\"./test_images/luffy_pose.png\")\n",
    "    control2=load_image(\"./test_images/posecanny.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ptp_utils_max_merge as ptp_utils\n",
    "def run_and_display(prompts, negative_prompts, latent=None, run_baseline=False, generator=None, control1=None,control2=None,  results_dir='results'):\n",
    "\n",
    "    images, x_t = ptp_utils.text2image_ldm_stable(pipe, prompts, negative_prompts, latent=latent, num_inference_steps=50, guidance_scale=7.5, generator=generator, control1=control1,control2=control2,low_resource=False)\n",
    "\n",
    "    ptp_utils.view_images(images,results_dir=results_dir)\n",
    "    return images, x_t\n",
    "\n",
    "\n",
    "seed=1024\n",
    "g_cpu = torch.Generator().manual_seed(seed)\n",
    "\n",
    "latent = torch.randn(\n",
    "                (1, 4,512 // 8, 512 // 8),\n",
    "                generator=g_cpu,\n",
    "            )\n",
    "\n",
    "image, x_t = run_and_display(prompts,negative_prompts, latent=latent , run_baseline=False, generator=g_cpu, control1= control1, control2 = control2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusers_freeu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
